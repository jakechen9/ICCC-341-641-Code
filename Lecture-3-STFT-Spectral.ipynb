{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1740678647977,
     "user": {
      "displayName": "Jake Cheng",
      "userId": "16560389850986014027"
     },
     "user_tz": 480
    },
    "id": "ZkGegMXOqLI7"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import seaborn\n",
    "import numpy, scipy, matplotlib.pyplot as plt, IPython.display as ipd, sklearn\n",
    "import librosa, librosa.display\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SrHRy3Gmr-UZ"
   },
   "source": [
    "# Revisiting Fourier Transform, DFT and FFT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LaQR2ldhtBo5"
   },
   "source": [
    "Fourier Transform (FT)\n",
    "* The Fourier Transform decomposes a signal into a sum of sinewaves (frequencies).\n",
    "* It provides frequency-domain representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KY41kj9Qs4j7"
   },
   "source": [
    "Discrete Fourier Transform (DFT)\n",
    "* The DFT is the discrete version of the Fourier Transform, used for sampled signals.\n",
    "* It converts a finite-length signal into a sum of sinusoids of different frequencies.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "thxAukSd5IPG"
   },
   "source": [
    "Fast Fourier Transform (FFT)\n",
    "* An efficient algorithm for computing the DFT.\n",
    "* FFT speeds up the DFT by recursively splitting the computation into smaller DFTs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YTcwuIxqwtTr"
   },
   "source": [
    "Why DFT & FFT Alone is Not Enough?\n",
    "1. DFT gives global frequency content but does not show how frequencies change over time.\n",
    "2. If a signal is non-stationary (changing over time), DFT fails to capture that.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wY9uOjufreGg"
   },
   "source": [
    "# Short-Time Fourier Transform\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eeW9y-3qrhpF"
   },
   "source": [
    "Musical signals are highly non-stationary, i.e., their statistics change over time. It would be rather meaningless to compute a single Fourier transform over an entire 10-minute song."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kSaEvJf98uZ5"
   },
   "source": [
    "How STFT works?\n",
    "\n",
    "Instead of analyzing the entire signal at once, the STFT breaks it into short overlapping segments (frames) by multiplying the signal with a window function\n",
    "$w(n)$, which selects a small portion of the signal at a time. The Fourier Transform is then computed for each windowed segment, sliding across time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "atZOl70b68HH"
   },
   "source": [
    "Mathematically, the STFT is defined as:\n",
    "\n",
    "$$ X(m, \\omega) = \\sum_n x(n) w(n-m) e^{-j \\omega n} $$\n",
    "\n",
    "where:\n",
    "* $x(n)$ is the original signal.\n",
    "* $w(n)$ is the window function.\n",
    "* m determines the time position of the window.\n",
    "* ùúî represents the frequency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qlsKGHcx8UoR"
   },
   "source": [
    "As we increase $m$, we slide the window function $w$ to the right. For the resulting frame, $x(n) w(n-m)$, we compute the Fourier transform. Therefore, the STFT $X$ is a function of both time, $m$, and frequency, $\\omega$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 75,
     "output_embedded_package_id": "1O7uhWBRfegZK3q1kceofLHZLELnUpwEd"
    },
    "executionInfo": {
     "elapsed": 2560,
     "status": "ok",
     "timestamp": 1740680211254,
     "user": {
      "displayName": "Jake Cheng",
      "userId": "16560389850986014027"
     },
     "user_tz": 480
    },
    "id": "CkWvdNPF8T1R",
    "outputId": "3cdacf6b-b604-4c1e-b1f3-3e6f9a2bcc47"
   },
   "outputs": [],
   "source": [
    "x, sr = librosa.load('audio/brahms_hungarian_dance_5.mp3')\n",
    "ipd.Audio(x, rate=sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zcWsOWIRCmX_"
   },
   "source": [
    "[`librosa.stft`](https://librosa.org/doc/main/generated/librosa.stft.html) computes a STFT. We provide it a frame size, i.e. the size of the FFT, and a hop length, i.e. the frame increment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 338,
     "status": "ok",
     "timestamp": 1740678655543,
     "user": {
      "displayName": "Jake Cheng",
      "userId": "16560389850986014027"
     },
     "user_tz": 480
    },
    "id": "87eBU-iJ_Z6I"
   },
   "outputs": [],
   "source": [
    "hop_length = 512\n",
    "n_fft = 2048\n",
    "X = librosa.stft(x, n_fft=n_fft, hop_length=hop_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Q_-ErQTC7kb"
   },
   "source": [
    "To convert the hop length and frame size to units of seconds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1740678656023,
     "user": {
      "displayName": "Jake Cheng",
      "userId": "16560389850986014027"
     },
     "user_tz": 480
    },
    "id": "YUlewPb9CvSu",
    "outputId": "69b8e338-2ac5-4807-aba5-ebf0ddd3242d"
   },
   "outputs": [],
   "source": [
    "float(hop_length)/sr # units of seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1740678656173,
     "user": {
      "displayName": "Jake Cheng",
      "userId": "16560389850986014027"
     },
     "user_tz": 480
    },
    "id": "bA_nHwwSC9KT",
    "outputId": "ba48a21f-cbaa-4ece-a82a-d7b9f79dbb71"
   },
   "outputs": [],
   "source": [
    "float(n_fft)/sr  # units of seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x0ONR1JMDgmC"
   },
   "source": [
    "For real-valued signals, the Fourier transform is symmetric about the midpoint. Therefore, `librosa.stft` only retains one half of the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1740678656479,
     "user": {
      "displayName": "Jake Cheng",
      "userId": "16560389850986014027"
     },
     "user_tz": 480
    },
    "id": "GGmjYub9C_i2",
    "outputId": "bfe4fd9f-2238-4f13-d441-ff6607c2dcb2"
   },
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7e59i_q3Dm4r"
   },
   "source": [
    "This STFT has 1025 frequency bins and 9813 frames in time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_bMABUWSF961"
   },
   "source": [
    "## Spectrogram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ExZ6r5IXF_Oz"
   },
   "source": [
    "In music processing, we often only care about the spectral magnitude and not the phase content.\n",
    "\n",
    "The **spectrogram** shows the the intensity of frequencies over time. A spectrogram is simply the squared magnitude of the STFT:\n",
    "\n",
    "$$ S(m, \\omega) = \\left| X(m, \\omega) \\right|^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SDPTA3C4Gyxe"
   },
   "source": [
    "The human perception of sound intensity is logarithmic in nature. Therefore, we are often interested in the log amplitude:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 266,
     "status": "ok",
     "timestamp": 1740680643470,
     "user": {
      "displayName": "Jake Cheng",
      "userId": "16560389850986014027"
     },
     "user_tz": 480
    },
    "id": "JWpET2k7DlSu"
   },
   "outputs": [],
   "source": [
    "S = librosa.amplitude_to_db(abs(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4-XpZiiGG5yJ"
   },
   "source": [
    "To display any type of spectrogram in librosa, use [`librosa.display.specshow`](https://librosa.org/doc/main/generated/librosa.display.specshow.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 483
    },
    "executionInfo": {
     "elapsed": 13871,
     "status": "ok",
     "timestamp": 1740678671683,
     "user": {
      "displayName": "Jake Cheng",
      "userId": "16560389850986014027"
     },
     "user_tz": 480
    },
    "id": "UpgG8L23G4Y7",
    "outputId": "50d709e1-af38-46ea-afae-4674b347119c"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "librosa.display.specshow(S, sr=sr, hop_length=hop_length, x_axis='time', y_axis='linear')\n",
    "plt.colorbar(format='%+2.0f dB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "obgMnMmPIIf-"
   },
   "source": [
    "## Mel-spectrogram\n",
    "\n",
    "A Mel-spectrogram is a spectrogram where the frequency axis is transformed using the Mel scale, which mimics how humans perceive sound. Instead of using a linear frequency scale, it spaces frequencies more densely at lower frequencies and more sparsely at higher frequencies, reflecting human auditory perception."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0GGayeKjeH_t"
   },
   "source": [
    "`librosa` has some outstanding spectral representations, including [`librosa.feature.melspectrogram`](https://librosa.org/doc/main/generated/librosa.feature.melspectrogram.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1497,
     "status": "ok",
     "timestamp": 1740678673182,
     "user": {
      "displayName": "Jake Cheng",
      "userId": "16560389850986014027"
     },
     "user_tz": 480
    },
    "id": "NJwiCyvoI3Jo"
   },
   "outputs": [],
   "source": [
    "hop_length = 256\n",
    "S = librosa.feature.melspectrogram(y=x, sr=sr, n_fft=4096, hop_length=hop_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BfpxSzsYe2Jt"
   },
   "source": [
    "The human perception of sound intensity is logarithmic in nature. Therefore, like the STFT-based spectrogram, we are often interested in the log amplitude:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 26,
     "status": "ok",
     "timestamp": 1740678673216,
     "user": {
      "displayName": "Jake Cheng",
      "userId": "16560389850986014027"
     },
     "user_tz": 480
    },
    "id": "FvFu2wVNeT80"
   },
   "outputs": [],
   "source": [
    "logS = librosa.power_to_db(abs(S))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 483
    },
    "executionInfo": {
     "elapsed": 4273,
     "status": "ok",
     "timestamp": 1740678677497,
     "user": {
      "displayName": "Jake Cheng",
      "userId": "16560389850986014027"
     },
     "user_tz": 480
    },
    "id": "qSwI2cTJe6lB",
    "outputId": "caed3e4f-8790-410d-ed5f-dee4cf155f7f"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "librosa.display.specshow(logS, sr=sr, hop_length=hop_length, x_axis='time', y_axis='mel')\n",
    "plt.colorbar(format='%+2.0f dB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yxKGLHR1fRTD"
   },
   "source": [
    "\n",
    "* X-axis (Time) ‚Üí Shows how the sound evolves over time.\n",
    "* Y-axis (Mel Frequency Scale) ‚Üí Uses the Mel scale instead of Hz, making lower frequencies more detailed.\n",
    "* Color Intensity (Amplitude in dB) ‚Üí\n",
    " * Red areas ‚Üí Higher energy (louder sounds).\n",
    " * Blue areas ‚Üí Lower energy (quieter sounds).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UMO242xShUTi"
   },
   "source": [
    "## Applications of Mel-Spectrograms\n",
    "* Music Genre Classification ‚Äì Used in machine learning models to classify different music genres based on frequency patterns.\n",
    "* Speech Recognition ‚Äì Forms the basis of automatic speech recognition (ASR)systems like Siri and Google Assistant.\n",
    "* Speaker Identification ‚Äì Helps in identifying individuals based on their unique vocal features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sThD5v9jhn9i"
   },
   "source": [
    "### Why Use Mel-Spectrograms?\n",
    "* Aligns with Human Hearing ‚Äì The Mel scale reflects how we perceive sound, making it more meaningful for audio analysis.\n",
    "* Better Frequency Resolution at Lower Frequencies ‚Äì More detail in bass and speech-relevant frequencies, improving classification accuracy.\n",
    "* Reduces Dimensionality ‚Äì Compared to a raw spectrogram, it compresses high-frequency data, making it more efficient for machine learning.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vAWV7dUQjbjv"
   },
   "source": [
    "## Introduction to the Constant-Q Transform (CQT)\n",
    "\n",
    "Similar to the Mel-spectrogram, the Constant-Q Transform (CQT) introduces a logarithmic frequency scale, but it is specifically designed for music analysis. Instead of focusing on human perception like the Mel scale, CQT aligns frequencies with musical pitches, such as semitones and octaves. This makes it ideal for tasks like music transcription, pitch tracking, and harmonic analysis, where capturing musical structures is important."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "No2ft2HIjmGi"
   },
   "source": [
    "To plot a constant-Q spectrogram, will use [`librosa.cqt`](https://librosa.org/doc/main/generated/librosa.cqt.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 479,
     "status": "ok",
     "timestamp": 1740678677979,
     "user": {
      "displayName": "Jake Cheng",
      "userId": "16560389850986014027"
     },
     "user_tz": 480
    },
    "id": "ifZVKTiZe8ub"
   },
   "outputs": [],
   "source": [
    "fmin = librosa.midi_to_hz(36)\n",
    "C = librosa.cqt(x, sr=sr, fmin=fmin, n_bins=72)\n",
    "logC = librosa.amplitude_to_db(abs(C))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 483
    },
    "executionInfo": {
     "elapsed": 3301,
     "status": "ok",
     "timestamp": 1740678681281,
     "user": {
      "displayName": "Jake Cheng",
      "userId": "16560389850986014027"
     },
     "user_tz": 480
    },
    "id": "SDhgVzz1jtE1",
    "outputId": "d5db2e54-d23f-43a4-a6cf-03103eda9d75"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "librosa.display.specshow(logC, sr=sr, x_axis='time', y_axis='cqt_note', fmin=fmin, cmap='coolwarm')\n",
    "plt.colorbar(format='%+2.0f dB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qtAI__ukmYdk"
   },
   "source": [
    "* X-axis (Time in minutes:seconds)\n",
    " * Shows how the signal changes over time.\n",
    "* Y-axis (Musical Notes, C2 to C8)\n",
    " * Instead of Hz (as in a regular spectrogram or Mel-spectrogram), the frequency axis is mapped to musical pitch classes (e.g., C2, C3, etc.).\n",
    " * Each frequency bin corresponds to a musical semitone, making it easier to analyze harmonics and melodies.\n",
    "* Color Intensity (Amplitude in dB)\n",
    " * Red regions indicate high-energy frequencies (louder notes).\n",
    " * Blue regions indicate low-energy frequencies (softer or silent parts)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pWcRKgPNm_oC"
   },
   "source": [
    "# Spectral Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "msFWW-AsnA54"
   },
   "source": [
    "For classification, we're going to be using new features in our arsenal: spectral moments (centroid, bandwidth, skewness, kurtosis) and other spectral statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iV-5dQArnLHf"
   },
   "source": [
    "[*Moments*](https://en.wikipedia.org/wiki/Moment_(mathematics) is a term used in physics and statistics. There are raw moments and central moments.\n",
    "\n",
    "You are probably already familiar with two examples of moments: mean and variance. The first raw moment is known as the mean. The second central moment is known as the variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a03f8kLCnRFE"
   },
   "source": [
    "## Spectral Centroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 75
    },
    "executionInfo": {
     "elapsed": 406,
     "status": "ok",
     "timestamp": 1740681994669,
     "user": {
      "displayName": "Jake Cheng",
      "userId": "16560389850986014027"
     },
     "user_tz": 480
    },
    "id": "haVpjOlojvHZ",
    "outputId": "8683127f-f0a5-4b06-dbf3-7ade24219a34"
   },
   "outputs": [],
   "source": [
    "x, sr = librosa.load('audio/simple_loop.wav')\n",
    "ipd.Audio(x, rate=sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gEzDPqSqnZnI"
   },
   "source": [
    "The **spectral centroid** ([Wikipedia](https://en.wikipedia.org/wiki/Spectral_centroid)) indicates at which frequency the energy of a spectrum is centered upon. This is like a weighted mean:\n",
    "\n",
    "$$ f_c = \\frac{\\sum_k S(k) f(k)}{\\sum_k S(k)} $$\n",
    "\n",
    "where $S(k)$ is the spectral magnitude at frequency bin $k$, $f(k)$ is the frequency at bin $k$.\n",
    "\n",
    "Basically, the spectral centroid represents the \"center of mass\" of the spectrum, giving an idea of whether a sound is more low-pitched (bass-heavy) or high-pitched (treble-focused).\n",
    "\n",
    "[`librosa.feature.spectral_centroid`](https://librosa.org/doc/main/generated/librosa.feature.spectral_centroid.html) computes the spectral centroid for each frame in a signal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 40,
     "status": "ok",
     "timestamp": 1740682074041,
     "user": {
      "displayName": "Jake Cheng",
      "userId": "16560389850986014027"
     },
     "user_tz": 480
    },
    "id": "Al8A9scXnO0i",
    "outputId": "73fbda9f-17d9-4698-b385-3896685501a8"
   },
   "outputs": [],
   "source": [
    "spectral_centroids = librosa.feature.spectral_centroid(y=x, sr=sr)[0]\n",
    "spectral_centroids.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yintbKLCognd"
   },
   "source": [
    "Compute the time variable for visualization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1740682077920,
     "user": {
      "displayName": "Jake Cheng",
      "userId": "16560389850986014027"
     },
     "user_tz": 480
    },
    "id": "C7K2Wx2yoJOu"
   },
   "outputs": [],
   "source": [
    "frames = range(len(spectral_centroids))\n",
    "t = librosa.frames_to_time(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xVWBz13Bomha"
   },
   "source": [
    "Define a helper function to normalize the spectral centroid for visualization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1740682084338,
     "user": {
      "displayName": "Jake Cheng",
      "userId": "16560389850986014027"
     },
     "user_tz": 480
    },
    "id": "af3jNxi0oh-T"
   },
   "outputs": [],
   "source": [
    "def normalize(x, axis=0):\n",
    "    return sklearn.preprocessing.minmax_scale(x, axis=axis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xuyfX23doq0-"
   },
   "source": [
    "Plot the spectral centroid along with the waveform:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "executionInfo": {
     "elapsed": 461,
     "status": "ok",
     "timestamp": 1740682090405,
     "user": {
      "displayName": "Jake Cheng",
      "userId": "16560389850986014027"
     },
     "user_tz": 480
    },
    "id": "oH4_23L8ood8",
    "outputId": "dd88e2aa-a2c1-4005-e244-49871112eef5"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "librosa.display.waveshow(x, sr=sr, alpha=0.4)\n",
    "plt.plot(t, normalize(spectral_centroids), color='r') # normalize for visualization purposes\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cwFowA16owa6"
   },
   "source": [
    "Similar to the zero crossing rate, there is a spurious rise in spectral centroid at the beginning of the signal. That is because the silence at the beginning has such small amplitude that high frequency components have a chance to dominate. One hack around this is to add a small constant before computing the spectral centroid, thus shifting the centroid toward zero at quiet portions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 467
    },
    "executionInfo": {
     "elapsed": 409,
     "status": "ok",
     "timestamp": 1740678682181,
     "user": {
      "displayName": "Jake Cheng",
      "userId": "16560389850986014027"
     },
     "user_tz": 480
    },
    "id": "Cr-CYQkSpQ8v",
    "outputId": "db8dabbe-a39c-4527-b3dc-ffbb421ff884"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "spectral_centroids = librosa.feature.spectral_centroid(y=x+0.01, sr=sr)[0]\n",
    "librosa.display.waveshow(x, sr=sr, alpha=0.4)\n",
    "plt.plot(t, normalize(spectral_centroids), color='r') # normalize for visualization purposes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yiT9PqBorsL-"
   },
   "source": [
    "* X-axis (Time in seconds) ‚Üí Shows the progression of the audio signal.\n",
    "* Y-axis (Amplitude for waveform, Normalized Spectral Centroid in Red)\n",
    "* The light blue waveform represents the original audio signal.\n",
    "* The red line represents the spectral centroid, which indicates whether the signal‚Äôs energy is concentrated in low frequencies (bass) or high frequencies (treble).\n",
    "\n",
    "\n",
    "Observing the Relationship Between the Waveform and Centroid\n",
    "* Low spectral centroid (Red Line near bottom) ‚Üí Corresponds to sections where the signal has more low-frequency content (e.g., bass sounds, silence).\n",
    "* High spectral centroid (Red Line peaks) ‚Üí Occurs when higher frequencies dominate (e.g., percussive sounds, bright notes, or sharp attacks).\n",
    "* Sudden peaks in the red line ‚Üí Often align with sharp transients in the waveform, meaning these moments have high-frequency content."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pqkvDHhLp208"
   },
   "source": [
    "Task:\n",
    "\n",
    "* Load different audio clips (e.g., a bass-heavy sound vs. a bright, high-pitched sound).\n",
    "* Compute and plot the spectral centroid over time using librosa.feature.spectral_centroid.\n",
    "* Compare the centroid between different sounds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YC6stY5AqR0G"
   },
   "source": [
    "## Spectral Bandwidth\n",
    "\n",
    "Spectral Bandwidth measures the spread of frequencies around the spectral centroid, indicating how wide or narrow the frequency distribution is.\n",
    "\n",
    "* A low spectral bandwidth means the energy is concentrated around a few frequencies (e.g., a pure sine wave or a flute note).\n",
    "* A high spectral bandwidth means the energy is spread across many frequencies (e.g., noise, cymbals, or distorted guitar sounds).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 467
    },
    "executionInfo": {
     "elapsed": 1697,
     "status": "ok",
     "timestamp": 1740683768926,
     "user": {
      "displayName": "Jake Cheng",
      "userId": "16560389850986014027"
     },
     "user_tz": 480
    },
    "id": "7j3zx7__pRNx",
    "outputId": "8cb59aac-84a9-4ef0-f5c6-8de85917cdc9"
   },
   "outputs": [],
   "source": [
    "hop_length = 512\n",
    "n_fft = 4096\n",
    "spectral_bandwidth_2 = librosa.feature.spectral_bandwidth(y=x+0.01, sr=sr, hop_length=hop_length, n_fft=n_fft)[0]\n",
    "spectral_bandwidth_3 = librosa.feature.spectral_bandwidth(y=x+0.01, sr=sr, hop_length=hop_length, n_fft=n_fft, p=3)[0]\n",
    "spectral_bandwidth_4 = librosa.feature.spectral_bandwidth(y=x+0.01, sr=sr, hop_length=hop_length, n_fft=n_fft, p=4)[0]\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "librosa.display.waveshow(x, sr=sr, alpha=0.4)\n",
    "plt.plot(t, normalize(spectral_bandwidth_2), color='r')\n",
    "plt.plot(t, normalize(spectral_bandwidth_3), color='g')\n",
    "plt.plot(t, normalize(spectral_bandwidth_4), color='y')\n",
    "plt.legend(('p = 2', 'p = 3', 'p = 4'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8IfI03RYq5np"
   },
   "source": [
    "* X-axis (Time in seconds) ‚Üí Represents how the audio evolves over time.\n",
    "* Y-axis (Normalized Bandwidth Values) ‚Üí Higher values indicate a broader frequency spread, while lower values indicate a narrower spread.\n",
    "* Waveform (Light Blue in Background) ‚Üí Shows the amplitude of the original audio signal.\n",
    "\n",
    "The plot contains multiple colored lines corresponding to different p-values (p=2, 3, 4).\n",
    "\n",
    "* These p-values refer to different orders of the spectral bandwidth computation, which affect how frequency spread is measured.\n",
    "* Red (p=2) ‚Üí Captures a wider variation in bandwidth.\n",
    "* Green (p=3) & Yellow (p=4) ‚Üí Show slightly more smoothed version\n",
    "\n",
    "\n",
    "Observing the Relationship to the Audio Signal\n",
    "* Peaks in spectral bandwidth correspond to transients or sudden changes in the audio waveform (e.g., drum hits or sharp attacks).\n",
    "* Lower values occur in steady-state regions (e.g., sustained tones or silence).\n",
    "\n",
    "Key Takeaways\n",
    "* Spectral bandwidth increases during fast, noisy, or percussive sections.\n",
    "* It decreases during smooth, tonal, or steady portions.\n",
    "* Different p-values affect how the spread is measured, with lower p-values showing sharper changes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HY-FofrAsUtR"
   },
   "source": [
    "## Spectral Contrast\n",
    "\n",
    "Spectral Contrast measures the difference between peaks (high energy) and valleys (low energy) in the frequency spectrum. Unlike spectral centroid or bandwidth, which describe the overall distribution of energy, spectral contrast focuses on how much the energy varies between frequency bands.\n",
    "\n",
    "* High spectral contrast ‚Üí Common in percussive sounds (e.g., drums) or music with sharp transients.\n",
    "* Low spectral contrast ‚Üí Found in smooth, harmonic sounds (e.g., flute, sustained vocals)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1740678682803,
     "user": {
      "displayName": "Jake Cheng",
      "userId": "16560389850986014027"
     },
     "user_tz": 480
    },
    "id": "oRVRvm83qlrZ",
    "outputId": "d903b3d6-26f8-4a69-b6c8-8e71a97ca022"
   },
   "outputs": [],
   "source": [
    "spectral_contrast = librosa.feature.spectral_contrast(y=x, sr=sr)\n",
    "spectral_contrast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 448
    },
    "executionInfo": {
     "elapsed": 186,
     "status": "ok",
     "timestamp": 1740678682989,
     "user": {
      "displayName": "Jake Cheng",
      "userId": "16560389850986014027"
     },
     "user_tz": 480
    },
    "id": "nhdLFc9ise6X",
    "outputId": "36331eda-8b7c-4aff-a17b-7a1f33d19658"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "plt.imshow(normalize(spectral_contrast, axis=1), aspect='auto', origin='lower', cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pth-huuWswXO"
   },
   "source": [
    "* X-axis (Frames / Time Steps) ‚Üí Represents how the spectral contrast evolves over time.\n",
    "* Y-axis (Frequency Bands) ‚Üí Divides the frequency spectrum into multiple bands (e.g., low, mid, high frequencies). Each row represents one frequency band.\n",
    "* Color Intensity (Contrast in dB)\n",
    " * Red areas ‚Üí High spectral contrast (large difference between peaks and valleys).\n",
    " * Blue areas ‚Üí Low spectral contrast (smooth or harmonic sounds).\n",
    "\n",
    "\n",
    "\n",
    "Key Observations\n",
    "* Bright red areas correspond to sharp transients or percussive elements, where energy differences between peaks and valleys are large.\n",
    "* Blue areas suggest sustained or harmonic sounds, where spectral energy is more evenly distributed.\n",
    "* Spectral contrast varies across different frequency bands, meaning some bands show more dynamic energy changes than others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Nsy1m_ZtPQm"
   },
   "source": [
    "What This Tells Us About the Audio\n",
    "\n",
    "same task as before :))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1740684353470,
     "user": {
      "displayName": "Jake Cheng",
      "userId": "16560389850986014027"
     },
     "user_tz": 480
    },
    "id": "vDHL7b23shgU",
    "outputId": "27ea1a4a-e845-467c-905f-0fafcd17d433"
   },
   "outputs": [],
   "source": [
    "print(spectral_contrast[6].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mDCGhGTH3u86"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNh6f8swqJKW9kKTXnV3u8b",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (MIR)",
   "language": "python",
   "name": "mir-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
