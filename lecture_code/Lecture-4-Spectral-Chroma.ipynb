{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "authorship_tag": "ABX9TyNeKFdkNXwJ5JNAuPp/4cnz"
  },
  "kernelspec": {
   "name": "mir-venv",
   "display_name": "Python (MIR)",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "%matplotlib inline\n",
    "import seaborn\n",
    "import numpy, scipy, matplotlib.pyplot as plt, IPython.display as ipd, sklearn\n",
    "import librosa, librosa.display\n",
    "import numpy as np"
   ],
   "metadata": {
    "id": "NMQ9Q8Ol3KAj",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1741285107656,
     "user_tz": 480,
     "elapsed": 0,
     "user": {
      "displayName": "Jake Cheng",
      "userId": "16560389850986014027"
     }
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Revisiting STFT & Spectral Features"
   ],
   "metadata": {
    "id": "mHrZ5_7O3M4d"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Short-Time Fourier Transform (STFT)\n",
    "* The STFT addresses the limitation of the Fourier Transform by analyzing signals in localized time windows.\n",
    "* Instead of treating the entire signal as a single entity, STFT divides it into overlapping segments (windows) and applies the Fourier Transform to each segment.\n",
    "* This produces a time-frequency representation, allowing us to observe how different frequency components evolve over time.\n",
    "* The spectrogram is a visualization of STFT that shows the magnitude of frequencies over time.\n"
   ],
   "metadata": {
    "id": "3LTx7nak4se-"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Spectral Features\n",
    "* Spectral Centroid: Measures the \"center of mass\" of the spectrum, giving an indication of the perceived brightness of a sound.\n",
    "* Spectral Bandwidth: Measures the spread of frequencies around the spectral centroid, indicating how wide or narrow the frequency distribution is.\n",
    "* Spectral Contrast: Measures the difference between peaks (high energy) and valleys (low energy) in the frequency spectrum."
   ],
   "metadata": {
    "id": "3F6_sCJp5Iq0"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Spectral Rolloff\n",
    "**Spectral rolloff** is the frequency below which a specified percentage of the total spectral energy, e.g. 85%, lies. It helps identify how much of a sound's energy is in low vs. high frequencies.\n",
    "\n",
    "Think of it as, telling us where most of the energy is concentrated below a certain frequency.\n",
    "\n",
    "\n",
    "\n",
    "Purpose: It helps distinguish between harmonic (tonal) sounds and noisy (percussive) sounds by identifying where most of the energy in the frequency spectrum resides."
   ],
   "metadata": {
    "id": "CbLEfk8B6reY"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "x, sr = librosa.load('audio/simple_loop.wav')\n",
    "ipd.Audio(x, rate=sr)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 75
    },
    "id": "YfxRpZA37TvD",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1741285107690,
     "user_tz": 480,
     "elapsed": 34,
     "user": {
      "displayName": "Jake Cheng",
      "userId": "16560389850986014027"
     }
    },
    "outputId": "18a05656-74f2-42af-8347-9d31684990be"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "spectral_centroids = librosa.feature.spectral_centroid(y=x, sr=sr)[0]\n",
    "frames = range(len(spectral_centroids))\n",
    "t = librosa.frames_to_time(frames)"
   ],
   "metadata": {
    "id": "Vi2FInJ27adN",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1741285107690,
     "user_tz": 480,
     "elapsed": 1,
     "user": {
      "displayName": "Jake Cheng",
      "userId": "16560389850986014027"
     }
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Define a helper function to normalize the spectral centroid for visualization:\n",
    "def normalize(x, axis=0):\n",
    "    return sklearn.preprocessing.minmax_scale(x, axis=axis)"
   ],
   "metadata": {
    "id": "5G7UMbG-7og9",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1741285107698,
     "user_tz": 480,
     "elapsed": 8,
     "user": {
      "displayName": "Jake Cheng",
      "userId": "16560389850986014027"
     }
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "[`librosa.feature.spectral_rolloff`](https://librosa.org/doc/main/generated/librosa.feature.spectral_rolloff.html) computes the rolloff frequency for each frame in a signal:"
   ],
   "metadata": {
    "id": "2SjsXdox8wKA"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(14, 5))\n",
    "spectral_rolloff = librosa.feature.spectral_rolloff(y=x+0.01, sr=sr)[0]\n",
    "librosa.display.waveshow(x, sr=sr, alpha=0.4)\n",
    "plt.plot(t, normalize(spectral_rolloff), color='r')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 483
    },
    "id": "51eIOu7d7L_L",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1741285108253,
     "user_tz": 480,
     "elapsed": 555,
     "user": {
      "displayName": "Jake Cheng",
      "userId": "16560389850986014027"
     }
    },
    "outputId": "59ba80a7-0d02-424d-de1d-dceb3bd79315"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "The red curve represents the normalized spectral rolloff over time.\n",
    "* Higher spectral rolloff values (peaks in the red curve) indicate that more energy is concentrated in higher frequencies, meaning the sound is brighter or sharper.\n",
    "* Lower spectral rolloff values (valleys in the red curve) indicate that most energy is in the lower frequencies, meaning the sound is more bass-heavy or muted."
   ],
   "metadata": {
    "id": "MSWkq2wL8DA9"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# PLOT DIFFERENT SOUNDS AND TEST IT OUT\n"
   ],
   "metadata": {
    "id": "n8ZzYaOy7sah",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1741285108255,
     "user_tz": 480,
     "elapsed": 1,
     "user": {
      "displayName": "Jake Cheng",
      "userId": "16560389850986014027"
     }
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Spectral Flatness"
   ],
   "metadata": {
    "id": "9JQffeUicbz1"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Spectral flatness (or tonality coefficient) is a measure to quantify how much noise-like a sound is, as opposed to being tone-like.\n",
    "\n",
    "It is calculated as the ratio of the geometric mean to the arithmetic mean of the power spectrum.\n",
    "\n",
    "* High spectral flatness (close to 1) → The spectrum is flat, meaning the energy is spread evenly across frequencies (white noise, percussive sounds).\n",
    "* Low spectral flatness (close to 0) → The spectrum has peaks at certain frequencies, meaning it is more tonal (like a musical note or harmonic-rich signal)."
   ],
   "metadata": {
    "id": "i2_WE6Gvce7Q"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Compute Spectral Flatness\n",
    "spectral_flatness = librosa.feature.spectral_flatness(y=x)\n",
    "\n",
    "# Plot Spectral Flatness\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.semilogy(spectral_flatness.T, label='Spectral Flatness')\n",
    "plt.xlabel('Time Frames')\n",
    "plt.ylabel('Spectral Flatness')\n",
    "plt.title('Spectral Flatness Over Time')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 410
    },
    "id": "_VMsuEsEcwjo",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1741285109110,
     "user_tz": 480,
     "elapsed": 855,
     "user": {
      "displayName": "Jake Cheng",
      "userId": "16560389850986014027"
     }
    },
    "outputId": "f7dc237b-f602-41e2-ef08-baa70c42c3e0"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "* X-axis (Time Frames) → Represents the progression of the signal over time.\n",
    "* Y-axis (Spectral Flatness) → Measures how flat the spectrum is in each time frame.\n",
    "* Low values → The signal is tonal, meaning there are dominant frequencies (e.g., musical notes, voiced speech).\n",
    "* High values → The signal is noisy, meaning energy is distributed more evenly across frequencies (e.g., percussive sounds, unvoiced speech)."
   ],
   "metadata": {
    "id": "zRIZpRIPc3pD"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Chroms Features\n",
    "\n",
    "Revisit Constant-Q Transform (CQT)\n",
    "* Unlike the linear frequency spacing in STFT, CQT provides logarithmic frequency bins, making it better suited for musical analysis.\n"
   ],
   "metadata": {
    "id": "NOMGzE6p8iN2"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Chroma features represent the tonal content of music by grouping frequencies into 12 pitch classes (C, C#, D, ..., B). Unlike spectral representations, which focus on absolute frequency values, Chroma features capture harmonic and melodic structure\n",
    "\n",
    "Think of it as, the perceived \"musical color\" of a pitch, where notes an octave apart are considered the same chroma."
   ],
   "metadata": {
    "id": "Em7Bukbo-jmi"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "There are four chroma variants implemented in librosa: chroma_stft, chroma_cqt, chroma_cens and chroma_vqt"
   ],
   "metadata": {
    "id": "iwXRFHLz-9jj"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Chroma STFT"
   ],
   "metadata": {
    "id": "sHVbGV3IAgY4"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "chroma_stft performs short-time fourier transform of an audio input and maps each STFT bin to chroma."
   ],
   "metadata": {
    "id": "-Y4D3FPOAxMN"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Load audio\n",
    "y, sr = librosa.load(librosa.ex('trumpet'))\n",
    "ipd.Audio(y, rate=sr)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 75
    },
    "id": "CWPdgir5BaZO",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1741285109220,
     "user_tz": 480,
     "elapsed": 109,
     "user": {
      "displayName": "Jake Cheng",
      "userId": "16560389850986014027"
     }
    },
    "outputId": "b387c8ba-1c54-471a-8117-cbeb6e416119"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Compute Chroma STFT\n",
    "chroma_stft = librosa.feature.chroma_stft(y=y, sr=sr)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(14, 5))\n",
    "librosa.display.specshow(chroma_stft, y_axis='chroma', x_axis='time', cmap='coolwarm')\n",
    "plt.colorbar()\n",
    "plt.title('Chroma STFT')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Pitch Class')\n",
    "plt.show()\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "id": "ne7sX5AfAkr4",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1741285109608,
     "user_tz": 480,
     "elapsed": 387,
     "user": {
      "displayName": "Jake Cheng",
      "userId": "16560389850986014027"
     }
    },
    "outputId": "44ebb197-19c7-4793-b967-29d5dae2d983"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "* The x-axis represents time.\n",
    "* The y-axis represents pitch classes (C, C#, D, ..., B).\n",
    "* Brighter colors indicate higher energy for that pitch class at a given time.\n",
    "* Since STFT uses linear frequency spacing, it may not align perfectly with musical harmonics."
   ],
   "metadata": {
    "id": "GBeE9dHwBkXA"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Chroma CQT\n",
    "chroma_cqt uses constant-Q transform and maps each cq-bin to chroma."
   ],
   "metadata": {
    "id": "HpYCQyGXBt-A"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Compute Chroma CQT\n",
    "chroma_cqt = librosa.feature.chroma_cqt(y=y, sr=sr)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(14, 5))\n",
    "librosa.display.specshow(chroma_cqt, y_axis='chroma', x_axis='time', cmap='coolwarm')\n",
    "plt.colorbar()\n",
    "plt.title('Chroma CQT')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Pitch Class')\n",
    "plt.show()\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "id": "9Q3oX-zxBWe8",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1741285110222,
     "user_tz": 480,
     "elapsed": 607,
     "user": {
      "displayName": "Jake Cheng",
      "userId": "16560389850986014027"
     }
    },
    "outputId": "a3293f62-5f98-43f4-f54a-e20400b7a7e0"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "* The overall structure is similar to Chroma STFT, but:\n",
    " * The frequencies align better with musical pitch classes.\n",
    " * More stable representation, especially for harmonic sounds (like sustained notes).\n",
    " * Better suited for chord and key detection.\n"
   ],
   "metadata": {
    "id": "pbXYmO6gCYFa"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Chroma CENS\n",
    "\n"
   ],
   "metadata": {
    "id": "IYEVBnX8C6aI"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Chroma CENS is a variant of Chroma features, designed to be more robust to dynamics and timbre variations. It is derived from Chroma CQT, but with additional processing steps that improve its stability and effectiveness for applications like audio matching, cover song identification, and music retrieval."
   ],
   "metadata": {
    "id": "ods455vIDB2D"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Compute Chroma CENS\n",
    "chroma_cens = librosa.feature.chroma_cens(y=y, sr=sr)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(14, 5))\n",
    "librosa.display.specshow(chroma_cens, y_axis='chroma', x_axis='time', cmap='coolwarm')\n",
    "plt.colorbar()\n",
    "plt.title('Chroma CENS')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Pitch Class')\n",
    "plt.show()\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "id": "3rAJJSTHCO8g",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1741285110978,
     "user_tz": 480,
     "elapsed": 756,
     "user": {
      "displayName": "Jake Cheng",
      "userId": "16560389850986014027"
     }
    },
    "outputId": "68ad0a54-aa0f-405a-94dd-f0f6e08cdf95"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "* Looks similar to Chroma CQT, but:\n",
    " * Has smoother variations (less fluctuation in brightness).\n",
    " * Stronger tonal stability, meaning it captures the harmonic progression of a song rather than individual notes.\n",
    " * Useful for music similarity tasks because it generalizes well across different performances.\n"
   ],
   "metadata": {
    "id": "-0YOS0tcDkS9"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Why Use Chroma CENS?"
   ],
   "metadata": {
    "id": "tDDkvoAuDVI1"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Regular Chroma features capture the harmonic content of music, but they can be sensitive to changes in dynamics and instrument timbre. This means that two versions of the same song (e.g., a live performance and a studio recording) might have different Chroma representations due to differences in loudness and tonal balance."
   ],
   "metadata": {
    "id": "Vt2IDX2VDTdv"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Plot Chroma CQT\n",
    "plt.figure(figsize=(14, 5))\n",
    "librosa.display.specshow(chroma_cqt, y_axis='chroma', x_axis='time', cmap='coolwarm')\n",
    "plt.colorbar()\n",
    "plt.title('Chroma CQT (Before Processing)')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Pitch Class')\n",
    "plt.show()\n",
    "\n",
    "# Plot Chroma CENS\n",
    "plt.figure(figsize=(14, 5))\n",
    "librosa.display.specshow(chroma_cens, y_axis='chroma', x_axis='time', cmap='coolwarm')\n",
    "plt.colorbar()\n",
    "plt.title('Chroma CENS (After Processing)')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Pitch Class')\n",
    "plt.show()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 957
    },
    "id": "8raNBoyjGE_p",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1741285111880,
     "user_tz": 480,
     "elapsed": 901,
     "user": {
      "displayName": "Jake Cheng",
      "userId": "16560389850986014027"
     }
    },
    "outputId": "dd1d164c-ac84-40e6-eddd-0cde4b63da46"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Chroma VQT"
   ],
   "metadata": {
    "id": "bLlKph0qFbwY"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "chroma_vqt uses the Variable-Q Transform (VQT), which adapts its resolution based on frequency content.\n",
    "\n",
    "It provides better low-frequency resolution while maintaining fine detail in higher frequencies.\n",
    "\n",
    "It's more suitable for complex polyphonic music where different instruments overlap.\n",
    "\n",
    "You need to specify the intervals parameter explicitly, which defines the spacing of the frequency bins."
   ],
   "metadata": {
    "id": "2UCkf_j0Ff1t"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Compute Chroma VQT\n",
    "chroma_vqt = librosa.feature.chroma_vqt(y=y, sr=sr, intervals='equal')\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(14, 5))\n",
    "librosa.display.specshow(chroma_vqt, y_axis='chroma', x_axis='time', cmap='coolwarm')\n",
    "plt.colorbar()\n",
    "plt.title('Chroma VQT')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Pitch Class')\n",
    "plt.show()\n",
    "\n",
    "# Compute Chroma CQT\n",
    "chroma_cqt = librosa.feature.chroma_cqt(y=y, sr=sr)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(14, 5))\n",
    "librosa.display.specshow(chroma_cqt, y_axis='chroma', x_axis='time', cmap='coolwarm')\n",
    "plt.colorbar()\n",
    "plt.title('Chroma CQT')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Pitch Class')\n",
    "plt.show()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 957
    },
    "id": "ZXBj_bX-Fvud",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1741288652762,
     "user_tz": 480,
     "elapsed": 1316,
     "user": {
      "displayName": "Jake Cheng",
      "userId": "16560389850986014027"
     }
    },
    "outputId": "328bdc51-d0eb-4dae-8225-b7056e1a0aa8"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "* More detailed and adaptive than Chroma CQT.\n",
    "* Captures low-frequency harmonic structures more effectively.\n",
    "* Suitable for analyzing music with rich harmonic textures, such as classical and jazz.\n"
   ],
   "metadata": {
    "id": "4uz4pjceGmTp"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Summary\n",
    "\n",
    "* Chroma features help capture musical pitch content, independent of absolute frequency.\n",
    "* Chroma CQT is often preferred over Chroma STFT for music analysis because it aligns with musical pitch perception.\n",
    "* Chroma CENS is useful for comparing songs rather than detecting exact notes.\n",
    "* Chroma VQT provides an adaptive resolution, making it more detailed for analyzing complex harmonic textures."
   ],
   "metadata": {
    "id": "-2YTbwebGz1k"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Autocorrelation"
   ],
   "metadata": {
    "id": "_rXENCXiHAvy"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The [autocorrelation](http://en.wikipedia.org/wiki/Autocorrelation) of a signal describes the similarity of a signal against a time-shifted version (delayed version) of itself.\n",
    "For a signal $x$, the autocorrelation $r$ is:\n",
    "\n",
    "$$ r(k) = \\sum_n x(n) x(n-k) $$"
   ],
   "metadata": {
    "id": "3VIEjWuzK-6i"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this equation, $k$ is often called the **lag** parameter. $r(k)$ is maximized at $k = 0$ and is symmetric about $k$."
   ],
   "metadata": {
    "id": "-tS0jS-wLWsA"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The autocorrelation is useful for finding repeated patterns in a signal. For example, at short lags, the autocorrelation can tell us something about the signal's fundamental frequency. For longer lags, the autocorrelation may tell us something about the tempo of a musical signal."
   ],
   "metadata": {
    "id": "lXjna1PsLENo"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "x, sr = librosa.load('audio/c_strum.wav')\n",
    "ipd.Audio(x, rate=sr)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 75
    },
    "id": "hCdZrUEVNY0J",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1741285112468,
     "user_tz": 480,
     "elapsed": 79,
     "user": {
      "displayName": "Jake Cheng",
      "userId": "16560389850986014027"
     }
    },
    "outputId": "3019971e-0b74-446f-b447-7e580ce5a83a"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(14, 5))\n",
    "librosa.display.waveshow(x, sr=sr)\n",
    "plt.grid(True)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 465
    },
    "id": "1LVI5R2_FwS9",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1741285113024,
     "user_tz": 480,
     "elapsed": 555,
     "user": {
      "displayName": "Jake Cheng",
      "userId": "16560389850986014027"
     }
    },
    "outputId": "a00c5930-e0b1-4edb-be65-0807a978677a"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### `numpy.correlate`"
   ],
   "metadata": {
    "id": "cOA-7VVxMK8j"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "There are two ways we can compute the autocorrelation in Python. The first method is [`numpy.correlate`](http://docs.scipy.org/doc/numpy/reference/generated/numpy.correlate.html):"
   ],
   "metadata": {
    "id": "_itkPmG-MRIc"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Because the autocorrelation produces a symmetric signal, we only care about the \"right half\".\n",
    "r = numpy.correlate(x, x, mode='full')[len(x)-1:]\n",
    "print(x.shape, r.shape)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BkX181FNMUz9",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1741285114771,
     "user_tz": 480,
     "elapsed": 1747,
     "user": {
      "displayName": "Jake Cheng",
      "userId": "16560389850986014027"
     }
    },
    "outputId": "2335a210-290d-4893-96f3-6bd168b0c84f"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(14, 5))\n",
    "plt.plot(r[:10000])\n",
    "plt.xlabel('Lag (samples)')\n",
    "plt.xlim(0, 10000)\n",
    "plt.grid(True)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 465
    },
    "id": "pdzN0CFxMXbU",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1741285115181,
     "user_tz": 480,
     "elapsed": 409,
     "user": {
      "displayName": "Jake Cheng",
      "userId": "16560389850986014027"
     }
    },
    "outputId": "02058eec-adfe-48f3-c336-cc771701d711"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "* The x-axis represents the lag in terms of samples.\n",
    "* The y-axis represents the correlation strength between the original signal and its delayed (lagged) version.\n",
    "\n",
    "Initial Peak at Lag = 0\n",
    "\n",
    "* The first peak (not visible at the extreme left) corresponds to lag = 0, where the signal is perfectly correlated with itself. This is expected because any signal is always maximally correlated with itself at zero lag.\n",
    "\n",
    "Periodic Peaks Indicating Repeating Structures\n",
    "\n",
    "* These peaks indicate repeating patterns in the waveform, which could correspond to a fundamental frequency and its harmonics.\n",
    "* If the signal were a pure periodic waveform, the peaks would be evenly spaced, indicating a stable fundamental frequency.\n",
    "\n",
    "\n",
    "The gradual decrease in peak height means that as the lag increases, the correlation becomes weaker.\n",
    "\n",
    "The variations in peak heights and spacing suggest that the signal is not perfectly periodic.\n"
   ],
   "metadata": {
    "id": "q08RgQRaQx-m"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### `librosa.autocorrelate`"
   ],
   "metadata": {
    "id": "GPYR9BBvSRKD"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The second method is [`librosa.autocorrelate`](http://bmcfee.github.io/librosa/generated/librosa.core.autocorrelate.html#librosa.core.autocorrelate):"
   ],
   "metadata": {
    "id": "bbgqtZJdSTr0"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "r = librosa.autocorrelate(x, max_size=10000)\n",
    "print(r.shape)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Xav3a86NMe14",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1741285115190,
     "user_tz": 480,
     "elapsed": 9,
     "user": {
      "displayName": "Jake Cheng",
      "userId": "16560389850986014027"
     }
    },
    "outputId": "34b59d84-e72e-4613-f21e-f2fe8cdb24eb"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(14, 5))\n",
    "plt.plot(r)\n",
    "plt.xlabel('Lag (samples)')\n",
    "plt.xlim(0, 10000)\n",
    "plt.grid(True)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 465
    },
    "id": "Nc16rjoFNX15",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1741285115477,
     "user_tz": 480,
     "elapsed": 287,
     "user": {
      "displayName": "Jake Cheng",
      "userId": "16560389850986014027"
     }
    },
    "outputId": "55ed6f4a-f7d7-4de1-d3c7-5d90433445a0"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Pitch Estimation"
   ],
   "metadata": {
    "id": "FcoSCmRQSswD"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The autocorrelation is used to find repeated patterns within a signal. For musical signals, a repeated pattern can correspond to a pitch period. We can therefore use the autocorrelation function to estimate the pitch in a musical signal."
   ],
   "metadata": {
    "id": "tTav78rMSu2h"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "x, sr = librosa.load('audio/oboe_c6.wav')\n",
    "ipd.Audio(x, rate=sr)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 75
    },
    "id": "IO6bN1fASyp2",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1741285115524,
     "user_tz": 480,
     "elapsed": 47,
     "user": {
      "displayName": "Jake Cheng",
      "userId": "16560389850986014027"
     }
    },
    "outputId": "3eb34286-e506-4ee4-d269-3c7ff580ad39"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Compute and plot the autocorrelation:"
   ],
   "metadata": {
    "id": "G49D1Uu8S1Io"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "r = librosa.autocorrelate(x, max_size=5000)\n",
    "plt.figure(figsize=(14, 5))\n",
    "plt.plot(r[:200])\n",
    "plt.grid(True)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 445
    },
    "id": "u9BKcTaSS3eo",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1741285115897,
     "user_tz": 480,
     "elapsed": 373,
     "user": {
      "displayName": "Jake Cheng",
      "userId": "16560389850986014027"
     }
    },
    "outputId": "62fbe399-63e1-409f-a765-a68774bebd0c"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "The autocorrelation always has a maximum at zero, i.e. zero lag. We want to identify the maximum outside of the peak centered at zero. Therefore, we might choose only to search within a range of reasonable pitches:"
   ],
   "metadata": {
    "id": "qx1QrTaES8RJ"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "midi_hi = 120.0\n",
    "midi_lo = 12.0\n",
    "f_hi = librosa.midi_to_hz(midi_hi)\n",
    "f_lo = librosa.midi_to_hz(midi_lo)\n",
    "t_lo = sr/f_hi\n",
    "t_hi = sr/f_lo"
   ],
   "metadata": {
    "id": "K3oqmeCBTct6",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1741285115897,
     "user_tz": 480,
     "elapsed": 7,
     "user": {
      "displayName": "Jake Cheng",
      "userId": "16560389850986014027"
     }
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "t_lo and t_hi here are correspond to reasonable fundamental frequencies.\n",
    "\n"
   ],
   "metadata": {
    "id": "d9Uprv_NUzrN"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "print(f_lo, f_hi)\n",
    "print(t_lo, t_hi)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KmW2eCwZU_Jg",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1741285115897,
     "user_tz": 480,
     "elapsed": 1,
     "user": {
      "displayName": "Jake Cheng",
      "userId": "16560389850986014027"
     }
    },
    "outputId": "dada48d9-9661-411d-ec11-8a0fe40d2c5a"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Set invalid pitch candidates to zero:"
   ],
   "metadata": {
    "id": "jNWDY_tCWVry"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "r[:int(t_lo)] = 0\n",
    "r[int(t_hi):] = 0"
   ],
   "metadata": {
    "id": "Kpxcg6EaU_85",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1741285115897,
     "user_tz": 480,
     "elapsed": 1,
     "user": {
      "displayName": "Jake Cheng",
      "userId": "16560389850986014027"
     }
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "* r[int(t_lo)] = 0\n",
    " * This eliminates any peaks at very low lags, meaning extremely high-frequency candidates (above MIDI 120 / 8372 Hz) are removed.\n",
    " * High-frequency pitches correspond to very small lag values, which may contain harmonics or noise.\n",
    "\n",
    "* r[int(t_hi):] = 0\n",
    "\n",
    " * This removes autocorrelation peaks for very large lag values, eliminating any frequencies below MIDI 12 (16.35 Hz).\n",
    " * Large lag values correspond to very low-frequency candidates, which may not be musically relevant."
   ],
   "metadata": {
    "id": "jDAtaZ0hXe0j"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(14, 5))\n",
    "plt.plot(r[:1400])"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 463
    },
    "id": "1DPql7lKaFIj",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1741285116371,
     "user_tz": 480,
     "elapsed": 466,
     "user": {
      "displayName": "Jake Cheng",
      "userId": "16560389850986014027"
     }
    },
    "outputId": "5325dbd1-d569-4cf5-d005-67bc1f22f0d4"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "* X-axis (Lag in Samples): Represents time shifts in samples.\n",
    "* Y-axis (Autocorrelation Value): Represents how well the signal correlates with itself at different lags.\n",
    "* A clear periodic structure suggests a well-defined pitch.\n"
   ],
   "metadata": {
    "id": "WAmbaAXGaWO3"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finding the First Peak (Fundamental Period)"
   ],
   "metadata": {
    "id": "-sY3OOUWai33"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "t_max = r.argmax()\n",
    "print(t_max)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Sz_rXfYyaFiW",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1741285116373,
     "user_tz": 480,
     "elapsed": 1,
     "user": {
      "displayName": "Jake Cheng",
      "userId": "16560389850986014027"
     }
    },
    "outputId": "ed627108-9f1f-40bd-e0cf-c05c770ad4f9"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Why Does Finding the First Major Peak in the Autocorrelation Function (ACF) Represent the Pitch?\n",
    "\n",
    "The fundamental frequency (pitch) of a sound corresponds to how often the waveform repeats per second. Since autocorrelation measures similarity between a signal and a time-shifted version of itself, periodic signals will show peaks at intervals corresponding to their repetition rate. The first major peak in the ACF (after lag = 0) represents the fundamental period, which we use to determine pitch.\n",
    "\n"
   ],
   "metadata": {
    "id": "zWIXgoLxa9w4"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finally, estimate the pitch in Hertz:"
   ],
   "metadata": {
    "id": "CjwPFOWibRug"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "float(sr)/t_max"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "defBF799a8Zc",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1741285116374,
     "user_tz": 480,
     "elapsed": 1,
     "user": {
      "displayName": "Jake Cheng",
      "userId": "16560389850986014027"
     }
    },
    "outputId": "b88e5f0c-180b-4950-f236-9b535275843b"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Indeed, that is very close to the true frequency of C6:"
   ],
   "metadata": {
    "id": "F6tofEI0bYSi"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "librosa.midi_to_hz(84)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IEo_KKzWbUeg",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1741285116382,
     "user_tz": 480,
     "elapsed": 2,
     "user": {
      "displayName": "Jake Cheng",
      "userId": "16560389850986014027"
     }
    },
    "outputId": "87b071ca-8938-4c79-a26d-049ab5bc399e"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "r = librosa.autocorrelate(x, max_size=5000)\n",
    "midi_hi = 120.0\n",
    "midi_lo = 12.0\n",
    "f_hi = librosa.midi_to_hz(midi_hi)\n",
    "f_lo = librosa.midi_to_hz(midi_lo)\n",
    "t_lo = sr/f_hi\n",
    "t_hi = sr/f_lo\n",
    "r[:int(t_lo)] = 0\n",
    "r[int(t_hi):] = 0\n",
    "t_max = r.argmax()"
   ],
   "metadata": {
    "id": "vAzRKL73bZvG",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1741285116389,
     "user_tz": 480,
     "elapsed": 0,
     "user": {
      "displayName": "Jake Cheng",
      "userId": "16560389850986014027"
     }
    }
   },
   "outputs": [],
   "execution_count": null
  }
 ]
}
